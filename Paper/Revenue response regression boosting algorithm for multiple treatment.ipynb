{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revenue response regression boosting algorithm for multiple treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervisor: Robin M. Gubela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jessica Str√∂bel\n",
    "<br>\n",
    "Shih-Chi Ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github repo: https://github.com/dorisscma/Applied-Predictive-Analytics-Sose20\n",
    "<p>\n",
    "    The images are not properly shown here, but in the github repo, the complete version can be found under the paper folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Introduction](#Introduction)\n",
    "* [2. Literature Review](#Literature_Review)\n",
    "* [3. Multilple Treatment Uplift AdaBoost](#Multilple_Treatmetn_Uplift_AdaBoost)\n",
    "    * [3.1 Notation](#Notation)\n",
    "    * [3.2 Uplift Analogue for Prediction Error](#Uplift_Analogue_for_Prediction_Error)\n",
    "    * [3.3 Pseudo Code](#Pseudo_Code)\n",
    "* [4. Evaluation Method](#Evaluation_Method)\n",
    "    * [4.1 Uplift Curves](#Uplift_Curves)\n",
    "* [5. Results](#Results)\n",
    "    * [5.1 Our Dataset](#Our_Dataset)\n",
    "    * [5.2 Implementation](#Implementation)\n",
    "    * [5.3 Result](#Result)\n",
    "* [6. Conclusion](#Conclusion)\n",
    "    * [6.1 Summary](#Summary)\n",
    "    * [6.2 Limitations and Outlook](#Limitations_and_Outlook)\n",
    "* [7. Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction<a class=\"anchor\" id=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive models support decision-making by exploiting the underlying patterns in the data to extract insights for future use. In the real world, we often need to select one condition that gives us the \"best\" outcome. Examples could be treatment choice: for a certain disease, the doctors need to decide which medicine will be the most effective for the patients; or if we consider direct marketing campaigns where the customers receive emails for advertisements, the company would  like to send the customer the message that yields the highest revenue. Take the latter example, traditionally a classification model selects those customers who should be targeted. That is, the customers having the highest probabilities of  purchasing after receiving the email will be chosen.\n",
    "<p>\n",
    "    However, this is not what exactly desired - there are customers who buy products even without receiving any emails, and targeting them will not increase our revenue, instead, we lose the cost of conducting the marketing campaign on them. Also, there exists the chance that the customers get annoyed by the campaign and don't want to buy any products anymore even though they would without getting the promotion emails. Hence, we want to know the causal relationship between the campaign and the purchasing behaviour of the customers. And this is where uplift modeling comes into play.\n",
    "<p>\n",
    "    Uplift models were mainly used for conversion setting where the outcome variable is binary, for instance, the model tells us if a customer will buy a product or not, but there are not so many of them deal with continuous outcome, that is, instead of only predicting yes or no, the model gives prediction on how much money the customer is going to spend due to the campaign, or say, treatment. Except from the target variable, we also want to cope with multiple treatment cases instead of single treatment. Consider the email marketing example again, we now have different versions of email that are going to be sent to the potential target customers, in this case, we want to not only know if the email would increase the sales or not, but also which email would do so the best.\n",
    "<p>\n",
    "    On that account, the goal of this paper is to develop an uplift model that handles the case of both multiple treatments and continuous outcome. Our model has its foundation based on the Contextual Treatment Selection (CTS) developed by Zhao 2017 <font color=green>[13]</font>,  combined with AdaBoost algorithms. Boosting often dramatically improves performance of classification models, so we make use of this property and aim to improve the uplift model performance under multiple treatments and continuous outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Literature Review<a class=\"anchor\" id=\"Literature_Review\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uplift modeling is an approach used for estimating an effect of a treatment on the individual level. It models the incremental effect of a treatment on the target group and should help targeting only those people who buy because they are subject to a treatment and wouldn‚Äôt buy otherwise. With the goal of improving user experience and engagement, it is often widely used in the fields of direct marketing and product offering (Zhao and Harinen 2019) <font color=green>[14]</font>. Other fields of application are, for instance, in personalized medicine (Jaskowski and Jaroszewicz 2012 <font color=green>[7]</font>) or the banking or telecommunication sector (Radcliffe and Surry 2011 <font color=green>[10]</font>). Uplift models can be differentiated into two different types of models: conversion and revenue uplift models. In contrast to conversion models which aim at maximizing incremental sales, revenue uplift models strive at maximizing incremental revenue. This is especially reasonable when customers differ in their spending <font color=green>[3]</font>.\n",
    "<p>\n",
    "    A further distinction of uplift models can be made regarding the number of used treatments, for instance the number of different coupon values provided to the customers in a marketing campaign. While many uplift models covered in literature  discuss only single treatment, the focus is shifting more and more towards models considering multiple treatments. One reason herefore is the rather rare consideration of a single treatment in a real world business setting. Often companies want to compare different forms of promotions, for instance different channels or individual discount codes. Despite its relevance in practical applications, literature on multitreatment uplift is rather limited.\n",
    "<p>\n",
    "    The most basic form of measuring uplift is the Separate Model Approach (SMA), where two separate models are built independently from each other on the treatment and the control data. The SMA suggests to calculate the uplift by simply subtracting the estimated probability of the two separate models. Due to its simplicity of a separate model for both treatment and control group it is possible to extend it to multiple treatments, whereby a separate model is build for each treatment (Zhao et al. 2017 <font color=green>[13]</font>). A clear advantage of the SMA is its simplicity and intuitive clarity, which is why it is often used to calculate benchmarks. There is also no need to develop new algorithms since it works with any state-of-the-art machine learning algorithm like XGBoost (Chen and Guestrin 2016 <font color=green>[1]</font>). However, the performance suffers from focusing on building two separate models. As a consequence, the models cannot pick up relatively weak uplift signals compared to the purchase proportion in control and treatment group (Radcliffe and Surry 2011 <font color=green>[10]</font>) and is therefore often outperformed by other uplift approaches (Lo 2002 <font color=green>[8]</font>).\n",
    "<p>\n",
    "    Besides the SMA, another approach to model uplift is to model uplift directly through adapting existing machine learning algorithms like decision trees. A single model is used to predict uplift directly by considering both treatment and control group. As a result, it allows to optimize directly the uplift estimation. In 2012, Rzepakowsi and Jaroszewicz (2012)<font color=green>[7]</font> introduced a decision tree uplift model with multiple treatments and customized splitting and pruning criteria. The primary objective in their model for splitting a unified uplift decision tree is to in increase the divergence of outcome distribution between the treatments and between the treatment and the control group. To evaluate each possible split, measures of divergence between class distributions like the squared Euclidian distance, chi-squared divergenceand Kullback-Leibler divergence are applied.\n",
    "<p>\n",
    "    Guelman et al. (2012) <font color=green>[4]</font> extended this approach by using a Random Forest rather than a single decision tree. The result of the Random Forest uplift model can be calculated by simply taking the average from each tree. In addition, Zhao et al. (2017) <font color=green>[13]</font> proposed a new direct uplift algorithm, which is a tree-based ensemble algorithm. The unique splitting criterion of the so called Contextual Treatment Selection (CTS) algorithm builds the decision trees in a way that it directly maximizes the expected response on the training set. At each step of the tree-growing process, the split that brings the greatest increase in expected response is chosen. What is special about CTS is that it can handle both multiple treatments and continuous outcomes. \n",
    "<p> \n",
    "        Uplift models are not only being continuously improved in terms of their applicability regarding multiple treatments, but also becoming more and more sophisticated and accurate in their prediction. One obvious approach for improving the performance of any classifier is the usage of Boosting, which can enhance the performance of a model remarkably. The basic idea of boosting is to incorporate many weak classifiers to obtain a better result in the end. A widespread application of Boosting is the AdaBoost algorithm, which combines three basic ideas: First, many weak classifiers are combined to build a strong classifier. Second, some of the weak classifiers get to say more than the others in the final model and last but not least, each new simple classifier is built by taking the errors of the previous one into account (Freund and Schapire, 1994 <font color=green>[2]</font>). One successful attempt of adapting AdaBoost to uplift models is made by Soltys and Jaroszewicz (2018) <font color=green>[12]</font>. In their paper, they describe three properties that an uplift boosting algorithm should have. Based on that, they introduce three different uplift boosting algorithms, each satisfying two of the three properties. Nonetheless, the model proposed in this paper only deals with single treatment cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilple Treatment Uplift AdaBoost<a class=\"anchor\" id=\"Multilple_Treatment_Uplift_AdaBoost\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the goal of building a model that deals with both ‚Äúmultiple treatment‚Äù and ‚Äúcontinuous target variable‚Äù, we combined the CTS model from Zhao et al. (2017) [13] and AdaBoost algorithm, in pursuit of a better prediction. In order to incorporate boosting algorithms into uplift modeling, we found the paper from So≈Çtys and Jaroszewicz [12] especially helpful. The proposed model in this paper combined AdaBoost and single-treatment uplift, and inspired by this, we designed an Adaboost uplift model that handles multiple treatments. Hence, the algorithm proposed by us aims at predicting the best treatment for each observation and giving the predicted response under this predicted treatment. Before introducing the algorithm, we first describe the notation used throughout this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Notation<a class=\"anchor\" id=\"Notation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, upper case letters are used to denote random variables and lower case letters the realizations; boldface characters stand for vectors and normal typeface for scalars.\n",
    "1. <b>$X$ </b> is the feature vector and x is its realization. We used Superscripts to indicate specific features. For example, $X_j$ is the $j$ th feature in the vector and $x_j$ its realization.\n",
    "2. <b>$T$</b> represents the treatment. Assuming ùêæ different treatments encoded as ${1, . . . , K}$. The control group is indicated by $T= 0$.\n",
    "3. <b>$Y$</b> denotes the response and ùë¶ the realization. Throughout this paper we assume the larger the value of $Y$, the more desirable the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Uplift Analogue for Prediction Error<a class=\"anchor\" id=\"Uplift_Analogue_for_Prediction_Error\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost uses an iterative approach to learn from the mistakes of the previous classifiers, so in order to improve the model performance with AdaBoost algorithm, we need a definition of the prediction error.\n",
    "However, unlike standard classification, due to the innate restriction of uplift models - there is no true answer for the prediction since an observation can never be treated with different treatments at the same time, we are not able to clearly identify whether an uplift model correctly classified a given observation to a treatment or not. To deal with this issue, we defined an approximate proxy of classification error for our uplift model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Error_definition.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $t$ is the predicted best treatment,  $t^{(i)}$ the actual treatment, $\\hat{y}$ the predicted response, and $y_i$ the actual response.\n",
    "<p>\n",
    "The intuition behind this proxy is rather simple: if an observation belongs to a treatment group and the model predicts that it should receive another treatment $(t^{(i)} \\neq t)$, then the model is only correct if the expected response from the recommended treatment is higher than the original treatment $(\\hat{y} >  y_i)$, otherwise, the prediction is wrong.\n",
    "    <p>\n",
    "On the other hand, for the case that the model, based on CTS mechanism, suggests the same treatment as the observation was actually assigned, we know that the treatment prediction is based on the average response within a child node, that is, if an observation has an predicted response larger than its actual response, meaning that the observation might get this treatment recommendation just because if fell into a node that has a higher average response from other \"high-response observations\". This happens when misclassification happens, the observation does not belong to this specific treatment group. On the contrary, if the actual response is higher than the predicted response even when the recommended treatment is the same as the actual treatment, it means that the observation has just high actual response. Since the CTS selects the maximal average response already, there will be no other classifications that lead to even higher expected response. Hence, in this case, we see it as a correct classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pseudo Code<a class=\"anchor\" id=\"Pseudo_Code\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pseudo_code.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has a really similar general-structure to the original AdaBoost: after the first predictions are derived, a second model is devised to correct the error from the first model. By decreasing the weights of the wrongly predicted observations, each weak learner focuses on the error of the previous one. Therefore, we will assign weights to each data points, and readjust the weights after each iteration. On top of that, the weak learners are also given corresponding weights to their prediction power for the later-on aggregation in order to obtain final predictions.\n",
    "<p>\n",
    "From the pseudo code above, we can see that after assigning initial weights to the observations, we first fit a CTS model $h_m$ to the dataset $D^{t}$ and get the prediction for both treatment and expected response, then according to the prediction error defined before, we can obtain the model error rate --the proportion of wrongly predicted data points with regard to data weights $w_i$ from the formula <font color=blue>$1$</font> given in step  <font color=blue>$2b$</font>.\n",
    "<p>\n",
    "With the model error rate, we can compute the model weight. When aggregating all the models from each iterations to derive the final results, we want to have higher contribution from those models with stronger predictive power, and less impact from the weak ones. Thus, we design the model weight formula in <font color=blue>$2c$</font> in the way that the model error rate having a negative relationship with the model weight --the more wrong predictions a model has, the higher the error rate, and as a consequence, the smaller the model weight. Figure <font color=blue>$1$</font> is the graph for formula <font color=blue>$2c$</font>. Since the error rate $\\epsilon_m$ is the proportion, it will always be in between $0$ and $1$ as marked in blue, it can be seen that the model weight $\\beta_m$ will accordingly be also always between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model_weight.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before storing the results and going into the next iteration is then update the observation weights. Here, in order to have our algorithm focusing on the wrong predictions, we decrease the observations' weights when they are correctly predicted, on the other hand, we increase the weights for those observations wrongly predicted. Since the model weight  $\\beta_m$ is between $0$ and $1$, the observation weights get smaller when multiplied with it; and larger when multiplied with the reciprocal, here we have the observation weights multiplied with not only the reciprocal of the model weights $\\beta_m$, but the reciprocal of  $(1-\\frac{1}{2}\\beta_m)$ for a reason, and this is explained shortly. In addition to decrease the weight for correct predictions and increase the weight for wrong predictions, we strive to also retain one property from the original AdaBoost - when a stronger model, which means it has lower error rate, makes a wrong prediction, we increase the weight of that data point even more; correspondingly, when a weak model gives a correct prediction, we also decrease the weight even more. And this is the reason why we designed the update of the observation weight for wrong predictions in the way that it's multiplied with the reciprocal of  $(1-\\frac{1}{2}\\beta_m)$ instead of the reciprocal of $\\beta_m$.\n",
    "<p>\n",
    "The desired properties of the update functions are clearly shown in figure <font color=blue>$2$</font>. The orange line $w_r$ in the graph stands for update for wrong predictions and the blue line $w_c$ for the correct ones. From the graph we can see that $w_r$ is always bigger than $1$, resulting that the updated weights to grow, and the higher the $\\beta_m$, the stronger the learner, the faster the $w_r$ grows; on the contrary, $w_c$ is always between $0$ and $1$, which reduces the weights. Furthermore, since it's equal to $\\beta_m$, it has the innate characteristics that when $\\beta_m$ declines, it falls as well. This is consistent to our wish that the weights drops even more when even a weak model gives correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"update_weight.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Method<a class=\"anchor\" id=\"Evaluation_Method\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any predictive model, measuring the performance of an uplift model is essential. However, there is a big difference in the assessment of the performance. While it is possible to compare the actual and predicted outcome of each individual in a predictive model, it is impossible to do the same for an uplift model. An individual cannot receive both a treatment and no treatment at the same time (Radcliffe, 2007) <font color=green>[9]</font>, so we will never know the ground truth-optimal treatment for each individual. This problem of causal inference (Holland,  1968 <font color=green>[6]</font>) leads to the fact that other ways of evaluation are need for the uplift setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Uplift Curves<a class=\"anchor\" id=\"Uplift_Curves\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common measures to evaluate uplift are uplift curves and qini curves (Radcliffe 2007 <font color=green>[9]</font>; Rzepakowski and Jaroszewicz, 2010 <font color=green>[11]</font>}). Both of them were initially introduced for single treatments and need to be adjusted for the multiple treatments as in our case.\n",
    "<p>\n",
    "    For the evaluation of our model we will use the uplift curve, which is described in Gutierrez et al. (2017) <font color=green>[5]</font> as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(t) = (\\frac{Y_t^{(T)}}{N_t^{(T)}} - (\\frac{Y_t^{(C)}}{N_t^{(C)}}) * (N_t)^{(T)} + N_t^{(C)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $Y^T$ ($Y^C$) depict the sum of the treated (control) individual predicted outcomes and $N^T$ ($N^C$) the number of the treatment (control) group members per bin. \n",
    "The idea behind the formula is to rank the individuals in the treatment and control group according to their predictions and then compute the average prediction per decile. \n",
    "<br>\n",
    "To use the formula for multiple treatments, we combine all treated individuals into one big group, independent of their assigned treatment. As a result, only one treatment and one control group remain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results<a class=\"anchor\" id=\"Results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Our Dataset<a class=\"anchor\" id=\"Our_Dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation of the proposed AdaBoost uplift algorithm, data from a campaign of an online shoe retailer is used. The campaign ran for half a year and involves several campaign types. Each consumer visiting the website of the shoe retailer was randomly allocated to either one of 4 treatment groups or the control group. In the control group, the consumers receive no communications. In the treatment groups, the assigned users get either a 20%, 25%, 50% or 75% discount on their next purchase. The goal of the campaign is to motivate the website visitors to buy something on the website. In the campaign, there isn‚Äôt a minimum order value. Discounts can be used for every purchase no matter how low or high in value.\n",
    "<p>\n",
    "The data set consists of 133,595 observations and 21 variables. Each observation describes an individual session of a customer. The data set contains both numeric variables like the money spend at the sessions and categorical variables like the channels through which the website is visited (search, e-mail, affiliate‚Ä¶). Other variables include information about whether the consumer is already known by the online shoe retailer, which campaign type the consumer was assigned to or if the consumer has an item add to his shopping cart.\n",
    "    <p>\n",
    "Since the experimental setting of the data collection process was a Randomized controlled trial, best experimental conditions are given. Since the subjects are randomly assigned to the treatment or control groups, all known and unknown factors that might have an influence on the results, are present in all of the treatment and control groups. Therefore, differences in outcome can be considered as effects of the treatments. To get an overview over the treatment group assignment and the initial uplift of the data, table 1 provides some interesting statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Table_1_Descripitve_uplift_statistics_of_experimental_data.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Implementation<a class=\"anchor\" id=\"Implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the data set can be used for modeling, some preparations need to be made. After excluding some variables from the data set like the zip code or the last conversion timestamp, we use one hot encoding to represent the categorical variables as binary vectors. One hot encoding is one of the most common used tools for the transformation of categorical values to numeric values since most algorithms require numeric values as an input.\n",
    "For training and evaluation of the algorithm, 80% of the data set are used to train the model and 20% to evaluate it. To compare the results of our algorithm, we also use a Separate Model Approach (SMA) and the CTS algorithm. The SMA is based on a random forest as a base learner, which parameters are tuned using grid search. We choose 8 iterations for our multilple treatment uplift AdaBoost algorithm. For comparability, all models are build and evaluated on the same data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Result<a class=\"anchor\" id=\"Result\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the different models is summarized in the uplift curves of Figure <font color=blue>3</font>. A first look shows that the the different models perform clearly better than the random assignment. While the CTS algorithm has the best results, both CTS and SMA are outperforming the our algorithm. The uplift curves of all the models are not steadily increasing, meaning that targeting as many consumers as possible won‚Äôt lead to the best result. This is in line with marketing theory since at a certain point at targeting more and more people, also people who would have responded anyway or for whom the action actually has no impact will be targeted, leading to unnecessary contact costs. Another observation is that all curves are decreasing directly in the beginning. One possible explanation for this is that the models and are not able to target the right persons in the beginning as first customers. The models should always prioritize only target those customers that responded because of the action and wouldn‚Äôt have responded otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Evaluation_new.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the performance of the AdaBoost Uplift algorithm is only slightly better than a random assignment and didn't outperform other algorithms like CTS or even the SMA as we expected. Convinced by the concept fo the algotrithm, we try to understand the possible causes for the bad performance. An indicator for the bad performance was found when we took closer look at the predictions of the money spend from each customer. The predicted amount of money spend is increasing in each round steadily, which is not desired. One reason we could figure out that leads to this problem is the imbalance of the dataset, the dataset contains a lot of zero-spending observations,  combined with our error definition -- those predictions got assigned to a different treatment in real world from the one recommended by the model, and has a higher expected response than real response, which is zero in this case ($t^{i}\\neq  t$ and $\\hat{y} > y_{i}$), will be classified as correct predictions and hence get lower weight for the next round, or even dropped out by weigthed sampling. \n",
    "<p>\n",
    "The result from this unanticipated mechanism is escalation of the response prediction after each iteration since the zero-spending cases are either having lower weight or excluded. Even though there exists a similar effect from the other of the error prediction when $t^{i} =  t$ and $\\hat{y} > y_{i}$, the probability of having assigned to a different treatment from what the model suggested is higher than the probability of having the same treatment assigned and recommended due to the fact that we have multiple treatments in the setting. That is, the impact from the imbalance couldn't be off-set in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion<a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Summary<a class=\"anchor\" id=\"Summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seminar paper, our goal was to develop an uplift algorithm for multiple treatments and continuous outcome that incorporates boosting algorithms to further improve the performance of uplift models. Therefore, we developed an algorithm based on the CTS from Zhao et al. (2017) <font color=green>[13]</font> and the AdaBoost algorithm. In order to combine uplift models with Adaboost, we designed an uplift analogue for prediction error, and based on the predictions from each CTS shallow tree and the error label, Adaboost is used for adjusting observation weights for improvement of the whole ensemble. The proposed algorithm gives prediction of both the best treatment for each observation and the predicted response under this treatment.\n",
    "<p>\n",
    "    To evaluate our model, we compare it with the original CTS and SMA using the uplift curves. Unfortunately, the evaluation based on the dataset from the online shoe retailer yeilds unsatisfactory results. The performance of our model is only slightly better than the random assignment and was even worse than the SMA, not to mention CTS.\n",
    "    After some further checks, we found out that the imbalance dataset could be one reason that causes the problem. But more importanly, the weighted sampling we employ in our model could be the main reason why the whole algorithm is not working so well. Under weighted sampling, some observations with low weights are very likely to be dropped out after one iteration, and it means that these observations will not be included in the model afterwards at all, that is, they are lost. What's worse is that some of them might even go through only one itereation, so they are predicted by just a shallow CTS tree. This means that the data points might have had essentially different rounds of training under weighted sampling: some survied till the end, some only half, and some were basically lost after one round. This affects our model performance significantly on all aspects, incluing the predicted uplift, giving rise to the poor uplift curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Limitations and Outlook<a class=\"anchor\" id=\"Limitations_and_Outlook\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main struggles our model encountered was the dataset imbalance and the missing data points caused by weighted sampling. For the imbalance issue, employing the two-stage approach proposed by Gubela, Lessman and Jaroszewicz (2020) <font color=green>[15]</font> might help to prevent the problem. Apart from the increment of expected response, involving the balance condition <font color=green>[12]</font> could also improve the performance of the model. According to So≈Çtys and Jaroszewiczn, an uplift boosting algorithm is said to satisfy the balance condition if the total weight assigned to control group equals the total weight assigned to treatment group at each itereation. The balance property can prevent the situation that one of the groups gains much higher weight than the other one and push the weak learners to put the emphasis only on the treatment or control group instead of the differences between them. Nevertheless, the balance condition in this paper is for single treatment use cases, how to adjust it for multiple treatment models should be still decided.\n",
    "<p>\n",
    "    As for the drawback owing to weighted sampling, incorperating the weight assignment directly into CTS of Zhao et al. (2017) <font color=green>[13]</font> might solve the problem. When the average response in each child node is calculated, instead of taking the simple average, adopting weighted mean would also give the same outcome, but will not yield any left-out samples as in the weighted sampling case.\n",
    "<p>\n",
    "    Besides the model improvement, since the attributes of the dataset vary largely and the evaluation part in this paper is only based on one dataset, a better way to evaluate the algorithms is to test it on different datasets and also compare it with more other uplift models.\n",
    "<p>\n",
    "    The last point we would like to bring up is that, it is also interesting to consider the cost factor of the treatment in the multilple treatment uplift AdaBoost algorithm for future research. When sending discount coupons to customers, the corresponding costs induced also differ. A high discount significantly reduces the company's profit and therefore should also be considered in the customer targeting decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reference<a class=\"anchor\" id=\"Reference\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In <i>Pro-ceedings of the 22nd acm sigkdd international conference on knowledge discovery and datamining</i>, pages 785‚Äì794, 2016.\n",
    "<br>\n",
    "[2] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learningand an application to boosting. <i>Journal of computer and system sciences</i>, 55(1):119‚Äì139,1997.\n",
    "<br>\n",
    "[3] Robin Marco Gubela, Stefan Lessmann, Johannes Haupt, Annika Baumann, TillmannRadmer, and Fabian Gebert. Revenue uplift modeling. In <i>ICIS</i>, 2017.\n",
    "<br>\n",
    "[4] Leo Guelman, Montserrat Guill√©n, and Ana M P√©rez-Mar√≠n. Random forests for upliftmodeling: an insurance customer retention case. In <i>International Conference on Modelingand Simulation in Engineering, Economics and Management</i>, pages 123‚Äì133. Springer,2012.\n",
    "<br>\n",
    "[5] Pierre Gutierrez and Jean-Yves G√©rardy. Causal inference and uplift modelling: A reviewof the literature. In <i>International Conference on Predictive Applications and APIs</i>, pages1‚Äì13, 2017.\n",
    "<br>\n",
    "[6] Paul W Holland. Statistics and causal inference. <i>Journal of the American statisticalAssociation</i>, 81(396):945‚Äì960, 1986.\n",
    "<br>\n",
    "[7] Maciej Jaskowski and Szymon Jaroszewicz. Uplift modeling for clinical trial data. In <i>ICMLWorkshop on Clinical Data Analysis</i>, 2012.\n",
    "<br>\n",
    "[8] Chung-Mau Lo, Henry Ngan, Wai-Kuen Tso, Chi-Leung Liu, Chi-Ming Lam, Ronnie Tung-Ping Poon, Sheung-Tat Fan, and John Wong. Randomized controlled trial of transarte-rial lipiodol chemoembolization for unresectable hepatocellular carcinoma. <i>Hepatology</i>, 35(5):1164‚Äì1171, 2002.\n",
    "<br>\n",
    "[9] Nicholas J Radcliffe. Using control groups to target on predicted lift: Building and assessinguplift models. <i>Direct Marketing Analytics Journal</i>, 1(3):14‚Äì21, 2007.\n",
    "<br>\n",
    "[10] Nicholas J Radcliffe and Patrick D Surry. Real-world uplift modelling with significance-based uplift trees. <i>White Paper TR-2011-1, Stochastic Solutions</i>, pages 1‚Äì33, 2011.\n",
    "<br>\n",
    "[11] Piotr Rzepakowski and Szymon Jaroszewicz. Decision trees for uplift modeling. In <i>2010IEEE International Conference on Data Mining</i>, pages 441‚Äì450. IEEE, 2010.\n",
    "<br>\n",
    "[12] Micha≈Ç So≈Çtys and Szymon Jaroszewicz. Boosting algorithms for uplift modeling. <i>arXivpreprint arXiv:1807.07909</i>, 2018.\n",
    "<br>\n",
    "[13] Yan Zhao, Xiao Fang, and David Simchi-Levi. <i>Uplift Modeling with Multiple Treatmentsand General Response Types</i>, pages 588‚Äì596.\n",
    "<br>\n",
    "[14] Zhenyu Zhao and Totte Harinen. Uplift modeling for multiple treatments with cost opti-mization. In <i>2019 IEEE International Conference on Data Science and Advanced Analytics(DSAA)</i>, pages 422‚Äì431. IEEE, 2019.\n",
    "<br>\n",
    "[15] Gubela, Robin M., Stefan Lessmann, and Szymon Jaroszewicz. \"Response transformation and profit decomposition for revenue uplift modeling.\" <i>European Journal of Operational Research</i> 283.2 (2020): 647-661."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
